\chapter{Conclusion} \label{conclusion}

This study conducted a large-scale analysis of toxicity on Mastodon, examining patterns across 724 instances throughout 2024. While previous research focused primarily on centralized platforms, our work provides new insights into how toxicity manifests in decentralized social networks and how moderation practices may influence these patterns.

The temporal analysis revealed that toxicity levels on Mastodon fluctuate in response to major global events, particularly during politically charged periods like the 2024 U.S. election. The connection between real-world events and online toxicity underscores the importance of considering contextual factors when studying harmful content in social networks.

Our comparison of moderation practices showed significant differences in toxicity levels, suggesting that active moderation practices, such as adherence to the Mastodon Covenant or isolation from blocklisted instances, contribute to reduced toxicity. However, these results require careful interpretation, as Mastodon's decentralized structure seems to create moderation complexities that extend far beyond our four-category classification scheme.

This work contributes to our understanding of decentralized social networks in several ways. First, it demonstrates that transformer-based models can effectively analyze toxicity at scale in federated environments. Second, it provides empirical evidence that moderation practices can influence platform-wide toxicity patterns. Finally, our methodology offers a framework for future large-scale studies of decentralized platforms.

Future research could build on these findings by analyzing the full dataset, examining category-specific toxicity patterns, or investigating how user migration between instances affects toxicity distribution. As decentralized platforms continue to grow in popularity, understanding their unique dynamics will become increasingly important for fostering healthier online communities.