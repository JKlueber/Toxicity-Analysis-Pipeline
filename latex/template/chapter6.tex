\chapter{Discussion} \label{discussion}

The temporal analysis showed clear peaks in toxicity corresponding to major political and global events throughout 2024, particularly around the U.S. election period. The distribution of toxicity levels from highest in blocklisted instances to lowest in non-communicating ones suggests an influence of moderation practices on platform toxicity.

\paragraph{Interpretation of Key Findings}
The results indicate that our toxicity prediction pipeline successfully captured patterns in the Mastodon dataset, despite limitations in model selection. The clear distribution of toxicity levels across instance categories supports our hypothesis that decentralized moderation practices influence platform toxicity. The temporal peaks suggest that global events can trigger increased toxic discourse, particularly in politically charged contexts. These findings validate our approach of using transformer-based models for large-scale toxicity analysis in federated social networks.

\paragraph{Comparison with Previous Work}
Our findings align with and extend previous research on decentralized moderation. \citet{bono:2024} observed widespread blocklist usage across Mastodon instances and raised concerns about potential misuse for moderating instances that may not require intervention. Our analysis demonstrates that blocklisted instances exhibit higher mean toxicity levels, while instances not communicating with blocklisted content maintain the lowest toxicity. This evidence suggests that blocklists effectively moderate genuinely harmful instances when properly implemented. 

The temporal dimension of our analysis provides new insights into toxicity evolution during real-world events, complementing event-focused studies like \citet{fan:2022} on toxicity patterns during health crises.

\paragraph{Limitations}
Our study has several limitations that should be considered when interpreting the results.

\subparagraph{Model Selection}
We compared three toxicity detection models using a small labeled subset of 256 toots. While the F1 scores were suboptimal for all models, we prioritized prediction confidence, ultimately selecting the Detoxify Unbiased model. This small validation set may not fully represent the diversity of toxic content in our dataset. However, the model's performance in identifying toxicity trends across instances suggests it was suitable for our analysis goals.

\subparagraph{Data Scope}
The analysis used a 1\% subset (approximately 18 million toots) rather than the full dataset due to computational constraints. We omitted our planned deduplication and merging methods because the subset contained fewer duplicates (50\% versus 95\% in the full dataset). The LSH-based methods for deduplication and merging require optimization for full-dataset analysis, particularly regarding memory management during index queries.

\paragraph{Future Work}
Several promising directions emerge from this research:

First, analyzing the full dataset with optimized LSH methods would improve result reliability. The current 1\% subset, while substantial, loses information about small instances and may miss patterns visible only at full scale. Especially because the toxic instances seem to be smaller ones, as the larger ones often comitted to the Mastodon Covenant.

Second, exploring all seven toxicity labels from our model could reveal category-specific patterns. Our focus on overall toxicity provides a broad view, but deeper analysis of specific toxic behaviors (e.g., identity attacks versus threats) might yield more nuanced insights.

Third, there is an interesting pattern in the data that we could not analyze in this work. The number of toots marked as sensitive is very high on November~6, 2024, which is the day after the US election. This day is also the day with the highest toxicity prediction across all instances (Figure~\ref{sensitive-toots}). Since users often mark toxic content as sensitive to flag it for removal, further analysis could reveal how effective this moderation strategy is at actually reducing toxicity. 