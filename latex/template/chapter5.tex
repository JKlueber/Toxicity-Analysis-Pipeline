\chapter{Building Ray-Pipeline} \label{ray-pipeline}

The construction of the Ray pipeline for predicting toxicity in posts involved a multi-step process designed to efficiently process and analyze large-scale data. The pipeline begins by retrieving the raw data from Elasticsearch, which is the initial data source. Following this, the plaintext is extracted from the HTML code, ensuring that the textual data is clean and suitable for further processing. For efficient comparison and grouping of similar posts, a minHash is computed on the extracted plaintext. This minHash representation is used to group the posts based on Jaccard similarity, a measure of textual overlap, which helps identify and cluster posts with similar content. Now one big grouped dataset is created and will later be used to merge the toxicity predictions back to the original data.

For the toxicity prediction we build a smaller dataset by retaining only the essential columns, namely the plaintext and the group identifiers, while discarding all other unnecessary columns. Because in our study we just focus This filtered dataset is then subjected to toxicity prediction using the selected model, in this case, the Detoxify unbiased model, which assigns toxicity scores to each post across various categories.

After the toxicity predictions are completed, the results are merged back with the original, larger dataset, ensuring that the toxicity scores are aligned with the corresponding posts. Finally, the enriched dataset, now containing both the original data and the toxicity predictions, is written to a .parquet file for efficient storage and subsequent analysis. This structured pipeline not only streamlines the process of toxicity prediction but also ensures scalability and reproducibility, making it well-suited for analyzing large-scale datasets from online communities such as Mastodon.