\chapter{Introduction}\label{introduction}

Social media has become an essential part of modern life, enabling billions of people to share their stories, connect over shared interests, and participate in discussions on global events. This has resulted in the formation of large online communities. Unlike physical communities, online communities allow for easier and more immediate grouping of individuals, as joining a community often requires minimal effort; simply clicking a button or following a page \cite{ellison:2007}. This ease of access fosters the creation of diverse and dynamic social ecosystems, where users interact under shared norms, behaviors, and communication styles unique to each community.

However, the aggregation of large numbers of individuals in online communities also increases the likelihood of negative behaviors, such as toxicity. Toxicity in online spaces refers to harmful actions, including hate speech, racism, sexism, and other forms of discrimination. These behaviors are often exacerbated by the anonymity and reduced social inhibitions that characterize digital environments \cite{suler:2004}.

\begin{figure}[ht]
    \centering
    \fbox{%
      \parbox{0.8\textwidth}{%
        \centering
        "And two *racist* will light the fire. Of fucking course." \\
        "Fuck all you obedient slaves." \\
        "Rio had maybe five percent of the *racist* and *homophobic* from this disaster."
      }
    }
    \caption{Three toxic examples posted during the 2024 Paris Olympics opening ceremony on Mastodon. Severe harmful words got replaced with the kind of offence.}
\end{figure}

Such behaviors can disrupt community solidarity and harm individual users, making toxicity a significant challenge for social media platforms. To mitigate these issues, online communities establish rules and moderation systems to enforce acceptable behavior. Violations of these rules can result in penalties, such as bans or restrictions, depending on the platform's moderation policies \cite{nicholson:2023}.

To understand the dynamics of online communities and the challenges they face, this case study focuses on Mastodon, a decentralized alternative to traditional social media platforms like Twitter. The recent acquisition of Twitter by Elon Musk has highlighted the risks of centralized social media, where a single individual or entity can exert significant control over platform governance, content moderation, and user experience \cite{zia:2023}. Such centralization can lead to abrupt policy changes, increased misinformation, and heightened toxicity, prompting users to seek alternatives. Mastodon, as a decentralized and federated platform, offers a contrasting model where power is distributed across independently operated instances. However, this decentralization also introduces unique challenges, such as inconsistent moderation standards and the potential for fragmentation within the fediverse \cite{zia:2023}. By examining Mastodon, this study aims to explore how decentralized platforms address toxicity and community management while navigating the complexities of a federated ecosystem.

Like every social media platform, Mastodon offers the possibility to interact with other people by publishing posts, reacting to posts, or sharing posts. However, Mastodon is a federated social media platform. Federation refers to a special kind of decentralization. Traditional social media platforms, such as Twitter, Facebook, or Instagram, have a single central service that all users access. In contrast, Mastodon has multiple services, called instances, which are used by any number of people. These instances can communicate with each other and create a federated network. Users can freely choose an instance based on language, community rules, moderation policies, and topics of interest. Each instance is managed by its own administrators, who set and enforce local rules \cite{mastodon:docs}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{../material/network_models.jpg}
    \caption{From left to right: Centralized networks connect all through a single controlling hub; Federated networks organize nodes into semi-autonomous interconnected clusters; Distributed networks connect all nodes with multiple pathways. \cite{mastodon:docs}}
    \label{fig:network-models}
\end{figure}

\paragraph{Research Gap and Contribution}
Prior work has established foundational knowledge about Mastodon's structure \cite{zulli:2020,la_cava:2021}, moderation practices \cite{bono:2024,nicholson:2023}, and toxicity analysis methods \cite{fan:2022}. However, no study has systematically examined toxicity patterns across the entire Mastodon network while accounting for its federated architecture. Our research addresses this gap by conducting the first large-scale toxicity analysis of Mastodon. Our research addresses this gap by conducting the first large-scale toxicity analysis of Mastodon. We analyzed a 1\% subsample of 1.8~billion Mastodon posts (called ``toots'') across 1,000~instances collected throughout 2024. The results reveal two key findings: First, toxicity levels show significant spikes during major political and global events, particularly around the U.S.~election period. Second, active moderation practices correlate with lower mean toxicity levels on instances, demonstrating the effectiveness of decentralized moderation approaches.

\enlargethispage{\baselineskip}