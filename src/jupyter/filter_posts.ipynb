{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan\n",
    "from elasticsearch_dsl.query import Range, Term\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_posts(input_json, high_threshold=0.7, mid_threshold=0.3, limit=50, attribute='toxicity'):\n",
    "    # Initialize lists for the different toxicity levels\n",
    "    high_posts = []\n",
    "    mid_posts = []\n",
    "    low_posts = []\n",
    "\n",
    "    # Iterate through each post and categorize based on toxicity\n",
    "    for post in input_json:\n",
    "        toxicity_score = post['toxicity'][attribute]\n",
    "        \n",
    "        if toxicity_score > high_threshold and len(high_posts) < limit:\n",
    "            high_posts.append({'id': post['id'], 'level': 1, attribute: toxicity_score})\n",
    "        elif high_threshold >= toxicity_score > mid_threshold and len(mid_posts) < limit:\n",
    "            mid_posts.append({'id': post['id'], 'level': 0, attribute: toxicity_score})\n",
    "        elif toxicity_score <= mid_threshold and len(low_posts) < limit:\n",
    "            low_posts.append({'id': post['id'], 'level': -1, attribute: toxicity_score})\n",
    "\n",
    "        # Stop if all categories have enough posts\n",
    "        if (len(high_posts) >= limit and\n",
    "            len(mid_posts) >= limit and\n",
    "            len(low_posts) >= limit):\n",
    "            break\n",
    "\n",
    "    # Combine all posts into a single list\n",
    "    combined_posts = high_posts + mid_posts + low_posts\n",
    "\n",
    "    # Write to CSV file\n",
    "    with open(f'../../data/output/filtered_{attribute}_posts.csv', mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['id', 'level', attribute])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined_posts)\n",
    "    \n",
    "    all_ids = [post['id'] for post in combined_posts]\n",
    "\n",
    "    return all_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/ceph/storage/data-tmp/2024/po87xox/thesis-klueber/data/output/toxicity_results.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "ids = []\n",
    "ids.append(filter_posts(data, attribute=\"severe_toxicity\"))\n",
    "ids.append(filter_posts(data, attribute=\"obscene\"))\n",
    "ids.append(filter_posts(data, attribute=\"threat\"))\n",
    "ids.append(filter_posts(data, attribute=\"identity_attack\"))\n",
    "ids.append(filter_posts(data, attribute=\"toxicity\"))\n",
    "ids.append(filter_posts(data, attribute=\"insult\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_toxicity = filter_posts(data, attribute=\"toxicity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_HOST = \"https://elasticsearch.srv.webis.de\"\n",
    "ELASTIC_PORT = 9200\n",
    "ELASTIC_USER = \"po87xox\"\n",
    "# As a way to hide the password at least from the notebook, enter a path to a file here, which only contains the password for Elastic.\n",
    "ELASTIC_PASSWORD_FILE = Path(\"../../data/passwords/webis-elasticsearch.txt\").expanduser()\n",
    "INDEX = \"corpus_mastodon_statuses*\"\n",
    "\n",
    "OUTPUT_PATH = Path(\"../../data/output/\")\n",
    "\n",
    "# Limit the Elastic searches to a specific date range. Crawling started on 2023-12-21.\n",
    "DATE_AFTER = \"2023-12-01T00:00:00\"\n",
    "## Ca. \"2024-01-30T12:00:00\" is the time when a new version of the fediverse data was gahtered.\n",
    "DATE_BEFORE = \"2024-02-22T00:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Elastic.\n",
    "with ELASTIC_PASSWORD_FILE.open(\"r\") as f:\n",
    "    password = f.readline().strip(\"\\n\")\n",
    "es = Elasticsearch(\n",
    "    [ELASTIC_HOST],\n",
    "    port=ELASTIC_PORT,\n",
    "    http_auth=(ELASTIC_USER, password),\n",
    "    timeout=3000,\n",
    "    scheme=\"https\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'range': {'crawled_at': {'gte': '2023-12-01T00:00:00',\n",
       "   'lte': '2024-02-22T00:00:00',\n",
       "   'format': 'date_hour_minute_second'}}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the date query\n",
    "date_query = Range(crawled_at={\"gte\": DATE_AFTER, \"lte\": DATE_BEFORE, \"format\" : \"date_hour_minute_second\"})\n",
    "date_query.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "ids = list(set(list(chain.from_iterable(ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_search: Search = Search(using=es, index=INDEX).filter(date_query)\n",
    "response = base_search.query(\"terms\", _id=ids_toxicity).source([\"content\"]).extra(size=1000).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "\n",
    "def extract_text_from_hit(hit):\n",
    "    return extract_plain_text(\n",
    "        hit['_source']['content'],\n",
    "        main_content=True,\n",
    "        alt_texts=False,\n",
    "        preserve_formatting=False\n",
    "    )\n",
    "\n",
    "data = []\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    doc_id = hit['_id']\n",
    "    plain_text = extract_text_from_hit(hit)\n",
    "    data.append({\"id\": doc_id, \"content\": plain_text})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(\"../../data/output/texts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "first_csv = pd.read_csv('../../data/output/filtered_toxicity_posts.csv')\n",
    "\n",
    "second_csv = pd.read_csv('../../data/annotated.csv')\n",
    "\n",
    "merged_df = pd.merge(first_csv, second_csv, on='id')\n",
    "\n",
    "label_mapping = {\n",
    "    \"Non Toxic\": -1,\n",
    "    \"Toxic\": 1,\n",
    "    \"Not sure\": 0\n",
    "}\n",
    "merged_df['label'] = merged_df['label'].map(label_mapping)\n",
    "\n",
    "result_df = merged_df[['id', 'level', 'label', 'toxicity', 'content']]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('../../data/output/toxicity_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Genauigkeit der Vorhersagen für Non Toxic beträgt: 0.60\n",
      "Die Genauigkeit der Vorhersagen für Not sure beträgt: 0.52\n",
      "Die Genauigkeit der Vorhersagen für Toxic beträgt: 0.59\n",
      "Anzahl non toxic: 72\n",
      "Anzahl not sure: 23\n",
      "Anzahl toxic: 54\n",
      "Die Genauigkeit der Vorhersagen beträgt: 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     The kids are watching Sing. I’m peeling the sp...\n",
       "2     Every #caturday, you guys make me wish my apar...\n",
       "12    F**k. I cracked my iPad Air's screen. I have A...\n",
       "17    toroids are a wunnerful thang #AmateurRadio ⚡️...\n",
       "20    me, writing a chapter of my book in bright ora...\n",
       "21    Like I’m genuinely having fucking heart palpit...\n",
       "26        how much does a fucking cauliflower weigh lol\n",
       "29                                             hey #COG\n",
       "32    I'm watching an episode of World Wide Wrestlin...\n",
       "36                   take a shower you stinky (me @ me)\n",
       "40    @cas Wow!!! Fuck yeah!!! I’ve got 3 or 4 or 5 ...\n",
       "44    I’m sick of all this damn plastic. What do y’a...\n",
       "46              God TV on the Radio are so fucking good\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../data/output/toxicity_test.csv')\n",
    "\n",
    "df_non_toxic = df[df['label'] == -1] \n",
    "df_not_sure = df[df['label'] == 0]    \n",
    "df_toxic = df[df['label'] == 1] \n",
    "\n",
    "correct_predictions_non_toxic = (df_non_toxic['label'] == df_non_toxic['level']).sum()\n",
    "correct_predictions_not_sure = (df_not_sure['label'] == df_not_sure['level']).sum()\n",
    "correct_predictions_toxic = (df_toxic['label'] == df_toxic['level']).sum()\n",
    "\n",
    "accuracy_non_toxic = correct_predictions_non_toxic / df_non_toxic.shape[0]\n",
    "accuracy_not_sure = correct_predictions_not_sure / df_not_sure.shape[0]\n",
    "accuracy_toxic = correct_predictions_toxic / df_toxic.shape[0]\n",
    "\n",
    "print(f\"Die Genauigkeit der Vorhersagen für Non Toxic beträgt: {accuracy_non_toxic:.2f}\")\n",
    "print(f\"Die Genauigkeit der Vorhersagen für Not sure beträgt: {accuracy_not_sure:.2f}\")\n",
    "print(f\"Die Genauigkeit der Vorhersagen für Toxic beträgt: {accuracy_toxic:.2f}\")\n",
    "\n",
    "print(f\"Anzahl non toxic: {df_non_toxic.shape[0]}\")\n",
    "print(f\"Anzahl not sure: {df_not_sure.shape[0]}\")\n",
    "print(f\"Anzahl toxic: {df_toxic.shape[0]}\")\n",
    "\n",
    "df_binomial = df[((df['label'] == -1) | (df['label'] == 1)) & ((df['level'] == -1) | (df['level'] == 1))]\n",
    "\n",
    "correct_predictions = (df_binomial['label'] == df_binomial['level']).sum()\n",
    "accuracy = correct_predictions / df_binomial.shape[0]\n",
    "\n",
    "print(f\"Die Genauigkeit der Vorhersagen beträgt: {accuracy:.2f}\")\n",
    "\n",
    "#ids with wrong predictions\n",
    "df_binomial[df_binomial['label'] != df_binomial['level']]['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "first_csv = pd.read_csv('../../data/output/filtered_toxicity_posts.csv')\n",
    "\n",
    "second_csv = pd.read_csv('../../data/annotated_binary.csv')\n",
    "\n",
    "merged_df = pd.merge(first_csv, second_csv, on='id')\n",
    "\n",
    "label_mapping = {\n",
    "    \"Non Toxic\": 0,\n",
    "    \"Toxic\": 1,\n",
    "}\n",
    "\n",
    "merged_df['label'] = merged_df['label'].map(label_mapping)\n",
    "\n",
    "merged_df['toxicity'] = merged_df['toxicity'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "result_df = merged_df[['id', 'label', 'toxicity', 'content']]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_df.to_csv('../../data/output/toxicity_test_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Genauigkeit der Vorhersagen für Non Toxic beträgt: 0.71\n",
      "Die Genauigkeit der Vorhersagen für Toxic beträgt: 0.74\n",
      "Anzahl non toxic: 84\n",
      "Anzahl toxic: 65\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/output/toxicity_test_binary.csv')\n",
    "\n",
    "df_non_toxic = df[df['label'] == 0] \n",
    "df_toxic = df[df['label'] == 1] \n",
    "\n",
    "correct_predictions_non_toxic = (df_non_toxic['label'] == df_non_toxic['toxicity']).sum()\n",
    "correct_predictions_toxic = (df_toxic['label'] == df_toxic['toxicity']).sum()\n",
    "\n",
    "accuracy_non_toxic = correct_predictions_non_toxic / df_non_toxic.shape[0]\n",
    "accuracy_toxic = correct_predictions_toxic / df_toxic.shape[0]\n",
    "\n",
    "print(f\"Die Genauigkeit der Vorhersagen für Non Toxic beträgt: {accuracy_non_toxic:.2f}\")\n",
    "print(f\"Die Genauigkeit der Vorhersagen für Toxic beträgt: {accuracy_toxic:.2f}\")\n",
    "\n",
    "print(f\"Anzahl non toxic: {df_non_toxic.shape[0]}\")\n",
    "print(f\"Anzahl toxic: {df_toxic.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     The kids are watching Sing. I’m peeling the sp...\n",
      "2     Every #caturday, you guys make me wish my apar...\n",
      "9                             i fear fish women want me\n",
      "12    F**k. I cracked my iPad Air's screen. I have A...\n",
      "13    I'M STILL COOKING I've been cooking for 100 ye...\n",
      "17    toroids are a wunnerful thang #AmateurRadio ⚡️...\n",
      "20    me, writing a chapter of my book in bright ora...\n",
      "21    Like I’m genuinely having fucking heart palpit...\n",
      "26        how much does a fucking cauliflower weigh lol\n",
      "29                                             hey #COG\n",
      "32    I'm watching an episode of World Wide Wrestlin...\n",
      "36                   take a shower you stinky (me @ me)\n",
      "40    @cas Wow!!! Fuck yeah!!! I’ve got 3 or 4 or 5 ...\n",
      "44    I’m sick of all this damn plastic. What do y’a...\n",
      "46              God TV on the Radio are so fucking good\n",
      "48    America: When you get Scottish,irish,polish,ge...\n",
      "54    🌟 < why are people tall ahaha. stop being tall...\n",
      "58    Gosh, it is quite scary that I have made a hyp...\n",
      "68    Got all the Caspar Babypants songs stuck in my...\n",
      "86    I had a set a while back, couldn't find it, bo...\n",
      "87    They'll tell you that it's okay to be trans bu...\n",
      "88    usluck.com/150658/secret-reaso Secret reason w...\n",
      "94    I still want to be able to cause a compile err...\n",
      "98    On this day eight years ago I got this charmin...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_non_toxic[df_non_toxic['label'] != df_non_toxic['toxicity']]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51     Re last boost: “find cool places”… that paid u...\n",
      "52     Sex noises on Match of the Day to Brian Cox’s ...\n",
      "53     Annual squishing of boobs for science complete...\n",
      "56     First photo is mum telling Joey to piss off. S...\n",
      "63     Just saw a button, 'consent options', re cooki...\n",
      "67     I've tried every #hack to get #AppleTV to cast...\n",
      "72     Cld still be cons, rah rah free mkt and family...\n",
      "76     My roommate doesn't like when I take sexy pics...\n",
      "78     Some of you have convinced yourselves that the...\n",
      "79     Ugh I hate that Moloko is part of the music in...\n",
      "81     Why do the female soloists who play with the s...\n",
      "85     heyy i will send as many nudes to men on reddi...\n",
      "92     Sex noises on Match of the Day to Brian Cox’s ...\n",
      "96     Sex noises on Match of the Day to Brian Cox’s ...\n",
      "97                          #nsfw Female nudity, erotics\n",
      "121    🚨 HELP 🚨 I still need $40 to pay my rent It's ...\n",
      "132    Did you think that you were safe from your pos...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_toxic[df_toxic['label'] != df_toxic['toxicity']]['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
