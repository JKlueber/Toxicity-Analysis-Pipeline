\chapter{Large-Scale Toxicity Analysis} \label{large-scale-analysis}
Having chosen the detoxify unbiased model, we then use a large-scale pipeline to assign seven toxicity values between 0 and 1 to each toot for further analysis. Afterwards we conduct a temporal analysis of toxicity trends across all of 2024. Our investigation further explored the impact of moderation policies and instance rules on average toxicity levels.

\section{Large-Scale Pipeline Architecture}
The pipeline is built using the Ray framework \cite{moritz:2018}, a distributed computing system that simplifies parallel and batch processing of large-scale data workloads. Ray provides high-level APIs for task scheduling and resource management, making it particularly suitable for batch-oriented data processing pipelines. The framework's ability to scale computations across clusters while maintaining fault tolerance makes it ideal for our large-scale toxicity analysis. Our pipeline consists of several stages: data reading, preprocessing, deduplication, toxicity prediction, and merging (Figure~\ref{fig:pipeline}), all efficiently coordinated through Ray's distributed execution model.

\paragraph{Prerequisites}
The Ray environment is configured with specific settings for parallelism, memory, and number of CPU's per task to ensure efficient processing of large datasets. We ran the tasks seperatly in parallel pipelines to avoid memory conflicts and cache our results between the stages. The pipeline is designed to run on a cluster with 507 CPU cores, 976GB of RAM, and 290GB of object store memory, by using 0.01 CPU cores per task, with a maximum of 100 tasks running in parallel. To ensure robustness, the pipeline is configured to retry failed tasks up to 10 times, with retry exceptions enabled to handle briefly flashing errors well. These settings ensure that the pipeline can handle large datasets efficiently on our cluster while minimizing resource conflicts.

For scalability, we used Ray's parallelized datasets\footnote{\url{https://docs.ray.io/en/latest/data/data.html}} and pandas\footnote{\url{https://pandas.pydata.org/docs/user\_guide/index.html}} data frames. The \textit{map\_batches}\footnote{url{https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map\_batches.html}} operation processes data in manageable chunks, distributing the workload across available cluster resources. Each batch is converted to a pandas DataFrame, enabling us to utilise pandas' vectorized operations and transformations on each subset of data.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{../material/pipeline.png}
    \caption{Data processing pipeline: 
    (\hyperref[step:reading]{1}) \textbf{Read}: Reading data from the ElasticSearch database; 
    (\hyperref[step:preprocess]{2}) \textbf{Preprocess}: Extracting plaintext and calculating MinHash (optional building subset); 
    (\hyperref[step:lsh]{3}) \textbf{Build LSH}: Building LSH index from processed data; 
    (\hyperref[step:dedup]{4}) \textbf{Deduplicate}: Near-duplicates are removed from processed data using the LSH index; 
    (\hyperref[step:toxicity]{5}) \textbf{Analyze Toxicity}: Performing toxicity analysis on deduplicated data; 
    (\hyperref[step:merge]{6}) \textbf{Merge}: Merging analyzed data with the original dataset using the LSH index. 
    Blue boxes represent processes, red indicates storage components, and yellow marks the external database.}
    \label{fig:pipeline}
\end{figure}

\paragraph{Reading Toots from Elasticsearch}\label{step:reading}
The pipeline's first stage retrieves the Mastodon data fields (described in Table~\ref{dataset-fields}) from Elasticsearch, applying the following filters:

\begin{enumerate}
    \item \textbf{Temporal scope}: Only toots posted during 2024
    \item \textbf{Instance selection}: From 1,000 fully crawled instances
    \item \textbf{Content filters}:
    \begin{itemize}
        \item \textbf{Original toots only} (excluding reblogs/boosts): \\
        Reblogs (boosts) were removed because only the ID and URL of the boosted toot are stored, not the original content. Fetching this content separately would introduce unnecessary complexity.
        
        \item \textbf{Text-only content} (removing toots with media attachments): \\ 
        Approximately~18\% of toots contain media attachments (mostly images). Since our toxicity detection models analyze only text, we excluded these toots to maintain consistency.
        
        \item \textbf{English-language labeled content}: \\
        Our analysis focuses on English-language toots to ensure compatibility with the toxicity models, which are optimized for English text.
    \end{itemize}
\end{enumerate}

We use the ray\_elasticsearch\footnote{\url{https://github.com/janheinrichmerker/ray-elasticsearch}} library for efficient Elasticsearch queries. The retrieved data contains the fields described in Table~\ref{dataset-fields}, including toot identifiers, content, instance information, and metadata flags. To avoid overloading the Elasticsearch cluster, we read data directly into local storage using the Parquet\footnote{\url{https://github.com/apache/parquet-java}} file format for efficient storage.

\paragraph{Language Detection using FastText}\label{step:language-detection}
Because the language labels provided by Mastodon are based on the users main language, there were still non-English toots in our subset. However, the detoxify unbiased model is only trained for English toxicity detection. To ensure only English texts are processed, a language detection step is performed using the FastText\footnote{\url{https://huggingface.co/facebook/fasttext-language-identification}} model to predict each toot's language. We kept only those labeled as English for further analysis, thereby reducing the subset by approx 3 million toots and 205 instances, resulting in a final subset of 14,693,503 toots and 724 instances.
FastText is ideal for this task due to its use of subword information (character n-grams), which enables robust handling of informal language, misspellings, and slang common in social media texts. Its efficiency and accuracy in language detection ensure reliable filtering, even for short or noisy inputs \cite{joulin:2017}.

\subsection*{Handling Large Datasets by Minhash-based Deduplication and Merging}
Because of the high duplication rate of 95\% in the original dataset, we planned to deduplicate the data before analyzing it. After the analysis by our model we wanted to merge the results back into the original dataset. The deduplication and merging processes are based on MinHash signatures, which allow efficient identification of near-duplicate toots without having to compare every toot against each other in the dataset. In our actual analysis on the 1\% subset we directly analyzed the toots without deduplication and merging, because the subset contains less than 50\% duplicates and without the deduplication and merging our analysis was much faster. Nevertheless, we will explain the process here for future work.

\paragraph{Preperation for Minhash-based Deduplication and Merging}\label{step:preprocess} 
Before calculating the MinHash of every toot, the plaintext is extracted from the HTML content using the extract\_plain\_text function from resiliparse \cite{bevendorff:2018}. On the plaintext we can simply calculate the MinHash using the datasketch API\footnote{\url{https://ekzhu.com/datasketch/documentation.html\#minhash}}. The MinHash provides an efficient way to estimate the Jaccard similarity between documents. The Jaccard similarity $J(A,B)$ between two sets $A$ and $B$ is defined as:

\begin{equation}
J(A,B) = \frac{|A \cap B|}{|A \cup B|}
\end{equation}

MinHash works by computing multiple hash values for each document's shingles (contiguous subsequences of words) and keeping only the minimum hash value for each permutation. The probability that the minimum hash values match for two toots equals their Jaccard similarity \cite{broder:2000}. Our implementation uses 64 permutations to balance storage requirements with similarity estimation accuracy.

\subsubsection{LSH Index Construction}\label{step:lsh} 
The deduplication and merging employ Locality-Sensitive Hashing (LSH) techniques to efficiently identify and remove near-duplicate toots from the dataset \cite{leskovec:2014}. The idea of LSH is to group similar toots into buckets through a process called banding. This approach works by dividing each document's MinHash signature into $b$ bands of $r$ rows each. For a document with a MinHash signature of length $n$, we ensure $b \times r \le n$. Each band is then processed separately through a hash function, and toots sharing identical hash values in any band are considered potential duplicates. By setting the jaccard similarity threshold to 0.9, the optimizer finds values for $b$ and $r$ so that the probability of two toots sharing a band is very high when their Jaccard similarity is $\geq90\%$. The threshold is chosen to 0.9 because jaccard similarity is just indicating word similarity but not semantic similarity. To not remove toots that are similar in words but not in meaning, we suggest using a higher threshold than the maybe more realistic threshold of 0.58 explored by \citet{wu:2020} for similarity in short texts. 

We built one LSH index for the entire dataset. If this index gets queried, the responses are all toot id's matching a jaccard similarity $\geq90\%$ to the queried toot.

\paragraph{MinHash-based Similarity Deduplication}\label{step:dedup} 
For each toot we query the LSH index by inserting one toot's minhash and receving all similar ids in the same bucket. Within each group of near-duplicates, we retain only the toot with the smallest id value, ensuring deterministic selection while removing duplicates. This approach guarantees that only one representative instance of each near-duplicate group remains in the final dataset.

\paragraph{MinHash-based Similarity Merging}\label{step:merge}
The LSH index is queried for each toot in our subset to find similar toots. Because we kept the smallest id in the deduplication, now the smallest id in the query results always matches an id in the deduplicated dataset. We concatenate the matched analyzed toot from the deduplicated dataset with the original toot we used for our query. The toots now contain all relevant information and the the dataset for further analysis is created.

\section{Toxicity Analysis}\label{step:toxicity}
We used the detoxify unbiased model to perform a zero-shot toxicity prediction on the 1\% subset. Zero-shot prediction means the model was not fine-tuned on Mastodon toots and makes predictions using only its pretrained knowledge. For each toot, the model predicts scores for all seven toxicity categories defined in Table~\ref{toxicity-categories}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{../material/toxicity_2024.png}
    \caption{Distribution of the daily mean value and a 7-day rolling average for the toxicity category over the year~2024. The main peaks are marked with events that occurred on that day, which may have influenced the toxicity value.}
    \label{toxicity-2024}
\end{figure}

\subsection{Peak Analysis of Toxicity Timeline} 
Figure~\ref{toxicity-2024} shows the distribution of the daily mean value and a 7-day rolling average for the toxicity category over the year~2024. Several peaks occur during major events, influencing the toxicity value. Overall, the toxicity value remains stable between 0.06 and 0.07 but drops toward the end of the year to 0.05. This drop may be explained by the increase in the total number of toots (Figure~\ref{toot-distribution}), indicating that toxic communities did not grow as much as non-toxic ones.

The most prominent peak coincides with the U.S. election results on November~6, 2024, when Donald Trump was elected president. The only clearly recognisable negative peak occurs on October~17, one day after Liam Payne's death. This peak likely results from a sudden increase in activity, with condolences and tributes eliciting positive sentiments. October~17 was the second most active day in 2024, probably due to the additional positive toots. Because it was a 'positive' rise, the mean toxicity level decreased in the end. Interestingly, the two days with the highest activity produced opposing toxicity peaks, as November~6 was the most active and most toxic day. 

Most peaks are related with key U.S. election events, possibly because the analysis focuses on English toots, and the majority of English speakers reside in the U.S. The largest non-election-related peak occurs on July~14, 2024, the day of the UEFA Euro~2024 and Copa América~2024 finals. Since this peak primarily falls under the threat category, it may reflect heightened aggression among football fans.

\subsection{Impact of Moderation Policies on Toxicity Levels}\label{moderation:categorization}

To explore the impact of moderation policies on toxicity levels, we scraped all instances linked in the picker from the Mastodon website\footnote{\url{https://joinmastodon.org/servers}}. All these instances committed to the Mastodon Covenant\footnote{\url{https://joinmastodon.org/covenant}}, which demands active moderation against racism, sexism, homophobia, and transphobia. Out of the 724 instances analyzed, 175 are part of the Mastodon Covenant. Following \citet{bono:2024} findings on blocklist-based moderation, we cross-referenced our instances with the \_unified\_tier0\_blocklist\footnote{\url{https://github.com/sgrigson/oliphant/blob/main/blocklists/README.md}} to uncover blocklisted instances from our crawl. Additionally we want to test whether toxicity is affected by communicating with blocklisted instances. We define \emph{communication} as occurring when users from blocklisted instances post on another instance.

We categorized the instances as follows to explore the impact of different moderation practices:
\begin{itemize}
\item \textbf{Moderated}: 175 Covenant instances (7,228,494 toots) representing explicit commitment to anti-toxicity policies.
\item \textbf{Blocklisted}: 13 blocklisted instances (92,403 toots) flagged by the community for harmful content, serving as a proxy for poorly moderated spaces.
\item \textbf{Communicating}: 286 instances (5,691,465 toots) communicating with blocklisted ones: Testing whether interaction with poorly moderated instances increases toxicity.
\item \textbf{Non-Communicating}: 250 instances (611,823 toots) not communicating with blocklisted ones: Providing a baseline for toxicity in isolated, but not Covenant-bound, communities.
\end{itemize}

\subsubsection{Formulating Hypotheses on How Moderation Policies Impact Toxicity}
We formulated four key hypotheses based on Mastodon's federated architecture and moderation mechanisms:

\begin{enumerate}
    \item \textbf{Moderated vs. Blocklisted:} \\
    We hypothesized that instances adhering to the Mastodon Covenant would show significantly lower toxicity scores than blocklisted instances. This expectation stems from the Covenant's explicit requirements for active moderation against racism, sexism, homophobia, and transphobia, while blocklisted instances represent spaces where such moderation is absent.

    \item \textbf{Moderated vs. Non-Communicating:} \\
    We expected no significant difference between toxicity scores in moderated and non-communicating instances, as both employ strategies to limit toxic content—either through active moderation or complete isolation from potentially toxic federated servers.

    \item \textbf{Communicating vs. Non-Communicating:} \\
    We hypothesized communicating instances to show higher toxicity scores than non-communicating ones, based on the premise that federation with poorly moderated servers allows toxic content to propagate through the network.

    \item \textbf{Blocklisted vs. Communicating:} \\
    We expected no significant difference between toxicity scores in blocklisted and communicating instances, as continuous interaction with toxic servers may lead to normalization and adoption of similar toxic behaviors.
\end{enumerate}


\subsubsection{Descriptive Analysis of Differences in Toxicity Scores Between Moderation Policies}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{../material/blocklist_vs_covenant_boxplot.png}
    \caption{Comparison of average toxicity levels of instances across four instance categories: 
    \textbf{Blocklisted}: Instances flagged for spreading inappropriate content; 
    \textbf{Communicating}: Instances interacting with blocklisted instances; 
    \textbf{Moderated}: Mastodon Covenant members with active moderation policies;
    \textbf{Non-communicating}: Instances not interacting with any blocklisted instances.;
    The middle line marks the median, the box spans the IQR (25th–75th percentiles), and whiskers extend to 1.5×IQR.}
    \label{blocklisted-vs-covenant}
\end{figure}

 The results in Figure~\ref{blocklisted-vs-covenant} offer initial evidence that moderation practices influence toxicity levels. The plot reveals the distribution of toxicity scores across four types of Mastodon instances, categorized by their moderation policies. First the toxicity level of each instance is calculated by taking the mean score of all there crawled toots. Then each boxplot represents the average toxicity level per moderation policy, revealing clear differences in toxicity across categories.

\textbf{Blocklisted} instances demonstrate the highest toxicity levels, with a median toxicity score (Mdn = 0.072, M = 0.091, SD = 0.047) exceeding all other categories. The large interquartile range (IQR) reflects the small sample size in this category. Although those 13 instances can not be considered representative of all blocklisted instances, they show a clear trend of high toxicity.

\textbf{Moderated} instances following the Mastodon Covenant display lower toxicity levels overall. Their median toxicity score (Mdn = 0.061, M = 0.063, SD = 0.015) is only slightly above that of non-communicating instances, and their IQR is narrower than the others. The compact boxplot indicates that the instances in this category are likely to be more homogenous in their moderation practices. This suggests that instances adhering to the Mastodon Covenant and actively moderating their content result in a less toxic environment.

Instances \textbf{communicating} with blocklisted instances as well as \textbf{Non-communicating} instances display a broad IQR and large whiskers distance. Therefore the results should be viewed with caution. Due to the imprecise categorization based on the communication with blocklisted instances, insances likely differ in terms of toxicity level.

Nethertheless \textbf{communicating} instances show moderately elevated toxicity. While their median toxicity score (Mdn = 0.065, M = 0.069, SD = 0.022) falls below blocklisted instances, it remains higher than the other categories. This supports the assumption that interaction with blocklisted instances being supposed to moderate poorly lead to increased toxicity. 

\textbf{Non-communicating} instances even display the lowest median toxicity score (Mdn = 0.053, M = 0.057, SD = 0.045). Although this might not be a perfect categorization, the overall trend indicates these instances maintain less toxic environments, likely due to their isolation from blocklisted content.

These findings support our hypotheses that moderation policies and federation behavior impact toxicity levels. Instances implementing active moderation or isolating from toxic environments show lower toxicity, while those interacting with blocklisted instances demonstrate increased toxic content.

\subsubsection{Statistical Analysis of Toxicity Differences}
To statistically analyze these observations, we employed independent samples t-tests to evaluate mean differences in toxicity scores between instance categories. The t-test assesses whether the observed differences between categories are statistically significant (unlikely to occur by random chance), with $\alpha$ < .05 as significance threshold. To complement this, we calculated Cohen's d as a standardized measure of effect size, representing the magnitude of differences between categories independent of sample size. The results are summarized in Table~\ref{statistical-comparisons}.

\paragraph{Moderated vs. Blocklisted Instances}
The test results confirmed our first hypothesis with statistically significant results ($t(14.26) = 2.264$, $p = .040$, $d = 1.40$) and a very large effect size. This substantial difference indicates that instances adhering to the Mastodon Covenant's moderation standards successfully maintain less toxic environments compared to blocklisted instances lacking such moderation.

\paragraph{Moderated vs. Non-Communicating Instances}
Contrary to our expectations, we found a statistically significant difference ($t(593.07) = 2.653$, $p = .008$, $d = 0.17$) between moderated and non-communicating instances, though with a small effect size. While significant, this minimal practical difference suggests our hypothesis about equivalent outcomes between these strategies requires refinement. The results imply that complete isolation from potentially toxic instances might be marginally more effective than relying solely on active content moderation.

\paragraph{Communicating vs. Non-Communicating Instances}
Our third hypothesis was strongly confirmed with highly significant results ($t(660.15) = 4.939$, $p < .001$, $d = 0.33$) and a small-to-medium effect size. This demonstrates that instances communicating with blocklisted servers exhibit noticeably higher toxicity levels than non-communicating instances, indicating that federation with blocklisted instances increases toxicity.

\paragraph{Blocklisted vs. Communicating Instances}
The statistical analysis failed to reject the null hypothesis ($t(14.30) = 1.784$, $p = .096$, $d=0.93$), indicating no significant difference in toxicity levels between blocklisted and communicating instances. While the observed means showed a descriptive difference, the lack of statistical significance means we cannot confidently conclude this represents a true population effect. This result leaves our hypothesis neither confirmed nor refuted - the data simply does not provide sufficient evidence to make a determination about the relationship between these instance types. The small sample size of blocklisted instances (n=13) particularly limits our ability to detect effects in this comparison.

\begin{table}[tb]
    \centering
    \begin{tabular}{@{}crrrrr@{}}
    \hline
    Comparison & t-statistic & df & p-value & Cohen's d & Effect Size \\
    \hline
    Moderated \\
    vs & 2.264 & 14.26 & \textbf{.040} & 1.40 & Very large \\
    Blocklisted \\
    \hline

    Moderated \\
    vs & 2.653 & 593.07 & \textbf{.008} & 0.17 & Small \\
    Non-communicating \\
    \hline

    Communicating \\
    vs & 4.940 & 660.15 & \textbf{<.001} & 0.33 & Medium \\
    Non-communicating \\
    \hline

    Moderated \\
    vs & 1.784 & 14.30 & .096 & 0.93 & Very large \\
    Communicating \\
    \hline
    \end{tabular}
    \caption{Summary of independent samples t-tests. Bold p-values indicate statistical significance ($\alpha$ < .05). Effect sizes interpreted using Cohen's d guidelines developed by \citet{funder:2019}.}
    \label{statistical-comparisons}
\end{table}
